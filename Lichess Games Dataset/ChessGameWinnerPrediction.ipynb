{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is my attempt to see if I can build a model to predict the winner(and draws) of a chess game(standard variation) on lichess given the players' ratings, the opening variations etc. And given a winner, which method of win. Initially I thought having a higher elo rating should in theory predict the outcome but expirience says otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has 16 attributes and 20059 data points. I will drop some of the attributes i deem to be irrelevant manually(user_ids , start time of game etc) and the a PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rated</th>\n",
       "      <th>turns</th>\n",
       "      <th>victory_status</th>\n",
       "      <th>winner</th>\n",
       "      <th>increment_code</th>\n",
       "      <th>white_rating</th>\n",
       "      <th>black_rating</th>\n",
       "      <th>opening_name</th>\n",
       "      <th>opening_ply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>False</td>\n",
       "      <td>13</td>\n",
       "      <td>outoftime</td>\n",
       "      <td>white</td>\n",
       "      <td>15+2</td>\n",
       "      <td>1500</td>\n",
       "      <td>1191</td>\n",
       "      <td>Slav Defense: Exchange Variation</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>16</td>\n",
       "      <td>resign</td>\n",
       "      <td>black</td>\n",
       "      <td>5+10</td>\n",
       "      <td>1322</td>\n",
       "      <td>1261</td>\n",
       "      <td>Nimzowitsch Defense: Kennedy Variation</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>61</td>\n",
       "      <td>mate</td>\n",
       "      <td>white</td>\n",
       "      <td>5+10</td>\n",
       "      <td>1496</td>\n",
       "      <td>1500</td>\n",
       "      <td>King's Pawn Game: Leonardis Variation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>61</td>\n",
       "      <td>mate</td>\n",
       "      <td>white</td>\n",
       "      <td>20+0</td>\n",
       "      <td>1439</td>\n",
       "      <td>1454</td>\n",
       "      <td>Queen's Pawn Game: Zukertort Variation</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>95</td>\n",
       "      <td>mate</td>\n",
       "      <td>white</td>\n",
       "      <td>30+3</td>\n",
       "      <td>1523</td>\n",
       "      <td>1469</td>\n",
       "      <td>Philidor Defense</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rated  turns victory_status winner increment_code  white_rating  \\\n",
       "0  False     13      outoftime  white           15+2          1500   \n",
       "1   True     16         resign  black           5+10          1322   \n",
       "2   True     61           mate  white           5+10          1496   \n",
       "3   True     61           mate  white           20+0          1439   \n",
       "4   True     95           mate  white           30+3          1523   \n",
       "\n",
       "   black_rating                            opening_name  opening_ply  \n",
       "0          1191        Slav Defense: Exchange Variation            5  \n",
       "1          1261  Nimzowitsch Defense: Kennedy Variation            4  \n",
       "2          1500   King's Pawn Game: Leonardis Variation            3  \n",
       "3          1454  Queen's Pawn Game: Zukertort Variation            3  \n",
       "4          1469                        Philidor Defense            5  "
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#specify data type for the columns in the data\n",
    "#type = {\"id\": str, \"rated\":bool, \"t1\":float, \"t2\":float,\"turns\":int32,\"victory_status\":str, \"winner\":str, \"inc_code\":str,\"white_id\":str,\"white_rating\":int32,\"black_id\":str,\"black_rating\":int32,\"moves\":str,\"pening_code\":str,\"opening_name\":str,\"opening_ply\":int32}\n",
    "\n",
    "\n",
    "#here i drop columns i deem useless e.g(id, user id's etc)\n",
    "cols_to_use = {1,4,5,6,7,9,11,14,15} #columns to use\n",
    "\n",
    "data = pd.read_csv(\"games.csv\", usecols=cols_to_use)#read in the csv file with game records\n",
    "#preview first 5 lines of loaded data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the target columns i wish to predict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "vstat_data = data.drop('victory_status', axis=1) #data to classify the victory_status\n",
    "wnr_data = data.drop(columns=['victory_status','winner'], axis=1) #data to classify the victory winner\n",
    "\n",
    "vstats = data['victory_status']\n",
    "winners = data['winner'] #targets for winner data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "encode data with numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnr_data['rated'] = wnr_data['rated'].replace({True:1, False:0}) #change the rated column to 1 and 0\n",
    "vstat_data['rated'] = vstat_data['rated'].replace({True:1, False:0}) #change the rated column to 1 and 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "opening_name will be one hot encode:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(wnr_data['opening_name']) #get the one hot encoding of the column\n",
    "one_hot = one_hot.join(pd.get_dummies(wnr_data['increment_code']))\n",
    "\n",
    "#drop the original column since it's been encoded\n",
    "wnr_data = wnr_data.drop('opening_name',axis=1)\n",
    "wnr_data = wnr_data.drop('increment_code',axis=1)\n",
    "\n",
    "#join the the encoded column with the rest of the data\n",
    "wnr_data = wnr_data.join(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot = pd.get_dummies(vstat_data['opening_name']) #get the one hot encoding of the column\n",
    "one_hot = one_hot.join(pd.get_dummies(vstat_data['increment_code']))\n",
    "one_hot = one_hot.join(pd.get_dummies(vstat_data['winner']))\n",
    "\n",
    "#drop the original column since it's been encoded\n",
    "vstat_data = vstat_data.drop('opening_name',axis=1)\n",
    "vstat_data = vstat_data.drop('increment_code',axis=1)\n",
    "vstat_data = vstat_data.drop('winner',axis=1)\n",
    "\n",
    "#join the the encoded column with the rest of the data\n",
    "vstat_data = vstat_data.join(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to split the data into training and testing data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#i shuffled them just incase they are sorted in an i missed.\n",
    "wnr_data.sample(frac=1) #fraction to return randomised = 1\n",
    "\n",
    "#80 20 split\n",
    "wnr_train, wnr_test = train_test_split(wnr_data, test_size=0.2)\n",
    "wnr_train_target, wnr_test_target = train_test_split(winners, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i shuffled them just incase they are sorted in an i missed.\n",
    "vstat_data.sample(frac=1) #fraction to return randomised = 1\n",
    "\n",
    "#80 20 split\n",
    "vstat_train, vstat_test = train_test_split(vstat_data, test_size=0.2)\n",
    "vstat_train_target, vstat_test_target = train_test_split(vstats, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First I will use a descision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "dTree = tree.DecisionTreeClassifier(criterion=\"entropy\", max_depth=50)\n",
    "dTree.fit(wnr_train, wnr_train_target)\n",
    "print(dTree.tree_.max_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the the predictions on the test_data and check the accuract score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48404785643070786\n"
     ]
    }
   ],
   "source": [
    "#predict on test data\n",
    "wnr_predns = dTree.predict(wnr_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "#get the accuracy score using the test targets\n",
    "print(accuracy_score(wnr_test_target, wnr_predns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for predicting victory status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5635593220338984\n"
     ]
    }
   ],
   "source": [
    "dTree.max_depth = 10\n",
    "dTree.fit(vstat_train, vstat_train_target)\n",
    "\n",
    "#predict on test data\n",
    "vstat_predns = dTree.predict(vstat_test)\n",
    "\n",
    "#get the accuracy score using the test targets\n",
    "print(accuracy_score(vstat_test_target, vstat_predns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The descision tree classifier produce poor results for both sets of targets. Pruning the wnr tree does not change the score. Prunning vstat data did improve it from 0.47 to 0.55 and max_depth=10 seems to be the sweet spot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next I will give Naive Bayes a go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "NBmodel = GaussianNB()\n",
    "\n",
    "#fit the wnr_data\n",
    "NBmodel.fit(vstat_train, vstat_train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the accuracy again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11340977068793619\n"
     ]
    }
   ],
   "source": [
    "#predict on test data\n",
    "NB_vstat_predns = NBmodel.predict(vstat_test)\n",
    "\n",
    "#get the accuracy score using the test targets\n",
    "print(accuracy_score(vstat_test_target, NB_vstat_predns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THe accuracy score is laughable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perhaps Logistic Regression might do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "#fit data\n",
    "logreg.fit(vstat_train, vstat_train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5682951146560319\n"
     ]
    }
   ],
   "source": [
    "vstat_predcns = logreg.predict(vstat_test)\n",
    "#get accuracy score\n",
    "print(accuracy_score(vstat_test_target, vstat_predcns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has just about the same performance as the pruned decision tree classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
